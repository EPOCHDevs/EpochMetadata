//
// Created by assistant on 08/24/25.
//
#include <epoch_script/transforms/core/config_helper.h>
#include <epoch_script/transforms/core/transform_registry.h>
#include <arrow/compute/api_vector.h>
#include <catch2/catch_test_macros.hpp>
#include <epoch_core/catch_defs.h>
#include <epoch_frame/factory/dataframe_factory.h>
#include <epoch_frame/serialization.h>
#include <index/datetime_index.h>
#include <vector>

#include <epoch_script/core/bar_attribute.h>
#include <epoch_script/core/constants.h>

using namespace epoch_core;
using namespace epoch_frame;
using namespace epoch_script;
using namespace epoch_script::transform;

namespace {

DataFrame read_hmm_input(const std::string &file) {
  auto path = std::filesystem::path(HMM_TEST_DATA_DIR) / file;
  auto df_res = epoch_frame::read_csv_file(path, epoch_frame::CSVReadOptions{});
  REQUIRE(df_res.ok());
  auto df = df_res.ValueOrDie();
  return df.set_index("index");
}

} // namespace

TEST_CASE("HMMTransform basic behavior (2 states)", "[hmm]") {
  const auto tf =
      epoch_script::EpochStratifyXConstants::instance().DAILY_FREQUENCY;

  // Load test input generated by python script: multi-column with UTC index
  auto df2 = read_hmm_input("hmm_input_2.csv");

  for (auto states : {2}) {
    DYNAMIC_SECTION("states=" << states) {
      // Build config for hmm transform with n_states option
      std::string type = "hmm_2";

      YAML::Node inputs_yaml;
      inputs_yaml[epoch_script::ARG] =
          std::vector<std::string>{"x", "y", "z"};
      YAML::Node options_yaml;
      // Note: n_states is NOT an option for hmm_2 - it's built into the type
      options_yaml["max_iterations"] = 1000;
      options_yaml["tolerance"] = 1e-5;
      options_yaml["compute_zscore"] = true;
      options_yaml["min_training_samples"] = 100;
      options_yaml["lookback_window"] = 0;
      auto cfg = run_op(type, std::string("hmm_test_") + std::to_string(states),
                        inputs_yaml, options_yaml, tf);

      auto tbase = MAKE_TRANSFORM(cfg);
      auto t = dynamic_cast<ITransform *>(tbase.get());
      REQUIRE(t != nullptr);

      auto out = t->TransformData(df2);

      // Output should have same number of rows as (possibly lookbacked) input.
      // Default lookback=0 -> full length
      REQUIRE(out.num_rows() == df2.num_rows());

      // Expected columns: state + individual state probabilities (state_0_prob, state_1_prob, ...)
      size_t expected_cols = 1 + states; // state + N probability columns
      REQUIRE(out.num_cols() == expected_cols);

      // Values checks
      // 1) state is integer in [0, N-1]
      auto state_col = out[cfg.GetOutputId("state")]
                           .contiguous_array()
                           .template to_vector<int64_t>();
      REQUIRE(state_col.size() == df2.num_rows());
      for (auto s : state_col) {
        REQUIRE(s >= 0);
        REQUIRE(s < states);
      }

      // 2) Individual state probability columns should exist (state_0_prob, state_1_prob)
      for (size_t i = 0; i < states; ++i) {
        auto prob_col_name = cfg.GetOutputId("state_" + std::to_string(i) + "_prob");
        REQUIRE(out.contains(prob_col_name));

        // Verify probabilities are between 0 and 1 (with epsilon tolerance for floating point)
        constexpr double epsilon = 1e-9;
        auto prob_vec = out[prob_col_name].contiguous_array().template to_vector<double>();
        for (auto p : prob_vec) {
          REQUIRE(p >= -epsilon);
          REQUIRE(p <= 1.0 + epsilon);
        }
      }
      INFO("HMM transform completed successfully with " << states << " states");
      INFO("Output columns: " << out.num_cols());
      for (const auto &col : out.column_names()) {
        INFO("Column: " << col);
      }
    }
  }
}

TEST_CASE("HMMTransform with lookback window", "[hmm]") {
  const auto tf =
      epoch_script::EpochStratifyXConstants::instance().DAILY_FREQUENCY;
  // 150 samples
  // Build a simple constant series with UTC index
  auto base = read_hmm_input("hmm_input_2.csv");

  // Only keep first 150 rows to guarantee enough length
  if (base.num_rows() > 150) {
    base = base.head(150);
  }
  // Build single-column input "x"
  auto df = base["x"].to_frame();

  // hmm_2 with lookback_window=100
  // NEW BEHAVIOR: Train on first 100 rows, predict on remaining 50 rows
  YAML::Node inputs_yaml;
  inputs_yaml[epoch_script::ARG].push_back("x");
  YAML::Node options_yaml;
  options_yaml["lookback_window"] = 100;
  // Note: n_states is NOT an option for hmm_2 - it's built into the type
  options_yaml["min_training_samples"] = 100; // satisfy constraint
  options_yaml["max_iterations"] = 1000;
  options_yaml["tolerance"] = 1e-5;
  options_yaml["compute_zscore"] = true;

  auto cfg = run_op("hmm_2", "hmm_lb", inputs_yaml, options_yaml, tf);
  auto tbase = MAKE_TRANSFORM(cfg);
  auto t = dynamic_cast<ITransform *>(tbase.get());
  REQUIRE(t != nullptr);

  auto out = t->TransformData(df);
  // With 150 total rows and lookback_window=100:
  // - Train on rows 0-99 (100 rows)
  // - Predict on rows 100-149 (50 rows)
  // Output should be 50 rows (prediction window only)
  REQUIRE(out.num_rows() == 50);
}

TEST_CASE("HMMTransform insufficient samples throws", "[hmm]") {
  const auto tf =
      epoch_script::EpochStratifyXConstants::instance().DAILY_FREQUENCY;
  // Fewer than default min_training_samples (100)
  // Use first 20 rows from input for consistent index/build
  auto base = read_hmm_input("hmm_input_2.csv");
  auto df = base["x"].iloc({0, 50}).to_frame();

  YAML::Node inputs_yaml;
  inputs_yaml[epoch_script::ARG].push_back("x");
  YAML::Node options_yaml; // keep defaults
  // Note: n_states is NOT an option for hmm_2 - it's built into the type
  options_yaml["max_iterations"] = 1000;
  options_yaml["tolerance"] = 1e-5;
  options_yaml["compute_zscore"] = true;
  options_yaml["min_training_samples"] = 100;
  options_yaml["lookback_window"] = 0;

  auto cfg = run_op("hmm_2", "hmm_small", inputs_yaml, options_yaml, tf);

  auto tbase = MAKE_TRANSFORM(cfg);
  auto t = dynamic_cast<ITransform *>(tbase.get());
  REQUIRE(t != nullptr);

  REQUIRE_THROWS(t->TransformData(df));
}
